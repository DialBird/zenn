---
title: "BigQueryã®ã‚¯ã‚¨ãƒªçµæœã‚’ã€Google Cloud Storage ã«CSVã¨ã—ã¦å‡ºåŠ›ã™ã‚‹"
emoji: "ğŸ‘‹"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: []
published: true
---

# ã‚„ã‚ŠãŸã„ã“ã¨
- [ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ Cloud Storage ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹](https://cloud.google.com/bigquery/docs/exporting-data?hl=ja#exporting_data_into_one_or_more_files)ã«ã‚ã‚‹ã‚ˆã†ãªã“ã¨ã‚’ã€Cloud Run ã® Node.js ã§ã‚„ã‚ŠãŸã„ã€‚
- ãŸã ã—å…¬å¼ã«ã‚ˆã‚‹ã¨ã€ä¸€ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«1GBã¾ã§ã—ã‹å‡ºåŠ›ã§ããšã€ãã®å ´åˆã«ã¯è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦å‡ºåŠ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
- Node.js ã® bigquery ã‚¯ãƒ©ã‚¤ãƒ³ãƒˆã‚’ä½¿ã£ã¦ã€ã©ã†ã‚„ã£ã¦ãã‚Œã‚’å®Ÿç¾ã™ã‚‹ã®ã‹ã®ã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã®ã§ã€è‡ªåˆ†ã§æ›¸ã„ãŸã€‚

# ã‚„ã‚Šæ–¹

## 1. BigQuery ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã€CSVã«å…¨éƒ¨å‡ºåŠ›

```ts
import { BigQuery } from '@google-cloud/bigquery'
import fs from 'fs'
import path from 'path'

const bigquery = new BigQuery()

/**
 * ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆã‚’è¡Œã†
 *
 * ä½¿ç”¨ä¸Šã€é›†è¨ˆçµæœã‚’ä¸Šæ›¸ãã™ã‚‹ã“ã¨ãŒã§ããªã„ãŸã‚ã€ã‚‚ã—é›†è¨ˆçµæœã‚’ä¸Šæ›¸ãã—ãŸã„å ´åˆã¯ã€
 * å‰ã‚‚ã£ã¦è©²å½“ã® latestAggregatedPeriodId ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ç¾¤ã‚’å‰Šé™¤ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹
 *
 * DELETE FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
 * WHERE latestAggregatedPeriodId = '${jsonData.latestAggregatedPeriodId}'
 */
const main = async () => {
  try {
    console.time('export-csv')
    // SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    const sqlPath = path.join(__dirname, 'sample.sql')
    const sqlQuery = fs.readFileSync(sqlPath, 'utf-8')
    const options = {
      query: sqlQuery,
      location: 'asia-northeast1',
      destinationTable: {
        projectId: PROJECT_ID,
        datasetId: DATASET_ID,
        tableId: TABLE_ID,
      },
      // æ›¸ãè¾¼ã¿ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®š (WRITE_TRUNCATE, WRITE_APPEND, WRITE_EMPTY)
      // æ¯å›æ–°ã—ãæ›¸ãç›´ã™
      writeDisposition: 'WRITE_TRUNCATE',
    }

    const [job] = await bigquery.createQueryJob(options)
    console.log(`Job ${job.id} started.`)

    // ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…ã¤
    await job.promise()

    console.log(`Job ${job.id} completed.`)

    const [extractJob] = await bigquery
      .dataset(DATASET_ID)
      .table(VOTER_POINT_AGGREGATION_TABLE_ID)
      .extract(admin.storage().bucket('bucket-name').file('sample-*.csv'), {
        location: 'asia-northeast1',
        format: 'CSV',
      })
    console.log(`Extract Job ${extractJob.id} created.`)

    console.timeEnd('export-csv')
  } catch (e) {
    console.error('Error', e)
  }
}

main()
```

## 2. BigQuery ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¯ã‚¨ãƒªçµæœï¼ˆãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨)ã‚’ CSVã«å‡ºåŠ›

- EXPORT DATA ã‚’ä½¿ã†

```sql
EXPORT DATA
  OPTIONS(
  uri='gs://bucket-name/sample-*.csv',
  format='CSV',
  header=true,
  overwrite=true) AS
SELECT
  *
FROM `project-id.dataset-id.table-id`;

```

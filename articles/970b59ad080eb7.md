---
title: "Cloud SQLã¸ã€Storageä¸Šã®è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãªã‚‰ã€Google APIçµŒç”±ãŒè‰¯ã•ã’"
emoji: "ğŸ˜Š"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ['gcp', 'cloudsql', 'cloudrun', 'cloudworkflow']
published: true
---

# çµè«–

- Cloud SQL ã«ã€è¤‡æ•°ã® CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãªã‚‰ã€Google API çµŒç”±ã®æ–¹ãŒã™ã‚“ãªã‚Šè¡Œã£ãŸï¼
- æœ€åˆã¯ Cloud Run x NodeJS ã§é ‘å¼µã‚ã†ã¨ã—ãŸãŒã€è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚‚ã†ã¨ã™ã‚‹ã¨ä½•æ•…ã‹å‡¦ç†ãŒæ­¢ã¾ã£ã¦ã—ã¾ã†

# å®Ÿç¾ã—ãŸã„ã“ã¨

- Cloud Storage ä¸Šã®ç‰¹å®šã®ãƒã‚±ãƒƒãƒˆãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã€è¤‡æ•°ã® CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Cloud SQL ã«ã¾ã¨ã‚ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸã„
- ï¼ˆè‡ªåˆ†ã®å ´åˆï¼‰5.5MB ãã‚‰ã„ã® CSV ãƒ•ã‚¡ã‚¤ãƒ«ãŒ 150 å€‹ï¼ˆå…¨ä½“ã§ä¸€å„„è¡Œï¼‰ã‚ã‚‹

# Cloud Run ã§é ‘å¼µã‚ã†ã¨ã—ãŸç—•è·¡

- ä»¥ä¸‹ãŒ CSV ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰

```ts
import { parse } from "csv-parse";
import admin from "firebase-admin";
import pg from "pg";

const connectionConfig: pg.PoolConfig = {
  host: process.env.PG_HOST,
  user: process.env.PG_USER,
  password: process.env.PG_PASSWORD,
  database: process.env.PG_DATABASE,
  port: 5432,
};

// Eventarcã‹ã‚‰å—ã‘å–ã£ãŸCSVæƒ…å ±ã‚’è¡¨ç¤ºã—ãŸã„
const bucketName = process.argv[2];

const main = async () => {
  if (!bucketName) {
    console.error("Invalid arguments");
    process.exit(1);
  }

  console.time("Cloud SQLã¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ");

  const bucket = admin.storage().bucket(bucketName);

  // ãƒã‚±ãƒƒãƒˆå†…ã® CSV ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
  const [csvFiles] = await bucket.getFiles({ matchGlob: `*.csv` });

  const pool = new pg.Pool(connectionConfig);

  const csvParser = parse({
    columns: true,
    skip_empty_lines: true,
    delimiter: [","],
  });

  const failedFiles: string[] = [];
  let totalRowCount = 0;

  /**
   * è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
   */
  const filePromises = csvFiles.map(async (csvFile) => {
    try {
      await new Promise<void>((resolve, reject) => {
        csvFile
          .createReadStream()
          .pipe(csvParser)
          .on("readable", async () => {
            const client = await pool.connect();

            client.on("error", (err) => {
              console.error("Error connecting to Cloud SQL:!!", err);
              reject(err);
            });

            try {
              await client.query("BEGIN");
              let record;
              let rowCount = 0;
              let bulkInsertValues = [];
              const chunkSize = 100000;

              while ((record = csvParser.read())) {
                bulkInsertValues.push([
                  record.id,
                  record.userId,
                  record.createdAt,
                  record.point,
                ]);

                rowCount++;
                totalRowCount++;

                if (bulkInsertValues.length >= chunkSize) {
                  const query = {
                    text: `INSERT INTO "ImportTest" ("id", "userId", "createdAt", "point") 
                           VALUES ${bulkInsertValues
                             .map(
                               (_, index) =>
                                 `($${index * 4 + 1}, $${index * 7 + 2}, $${
                                   index * 4 + 3
                                 }, $${index * 4 + 4})`
                             )
                             .join(", ")}`,
                    values: bulkInsertValues.flat(),
                  };
                  await client.query(query);
                  bulkInsertValues = [];
                }
              }

              if (bulkInsertValues.length > 0) {
                const query = {
                  text: `INSERT INTO "ImportTest" ("id", "userId", "createdAt", "point") 
                         VALUES ${bulkInsertValues
                           .map(
                             (_, index) =>
                               `($${index * 4 + 1}, $${index * 4 + 2}, $${
                                 index * 4 + 3
                               }, $${index * 4 + 4})`
                           )
                           .join(", ")}`,
                  values: bulkInsertValues.flat(),
                };
                await client.query(query);
              }

              await client.query("COMMIT");

              console.log(
                `${rowCount}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’Cloud SQLã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸã€‚`
              );
            } catch (err) {
              await client.query("ROLLBACK");
              console.error("Error connecting to Cloud SQL:", err);
              reject(err);
            } finally {
              client.release(); // æ¥ç¶šã‚’ãƒ—ãƒ¼ãƒ«ã«è¿”å´
            }
          })
          .on("error", (err) => {
            console.error("CSVãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼:", err);
            reject(err);
          })
          .on("end", async () => {
            resolve();
          });
      });
    } catch (err) {
      failedFiles.push(csvFile.name);
    }
  });

  // ä¸¦åˆ—ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
  await Promise.all(filePromises);

  await pool.end(); // æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’çµ‚äº†
  console.log("Pool.end called!");

  if (failedFiles.length > 0) {
    console.error("ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«: ", failedFiles);
  } else {
    console.log(
      `${csvFiles.length}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚`
    );
  }

  console.timeEnd(`${competitionRoundId}ã‹ã‚‰ã€Cloud SQLã¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ`);
};

main();
```

- æœ€åˆã¯ CSV ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¦ã€ãƒ¡ãƒ¢ãƒªçš„ãªåˆ¶ç´„ã«å¼•ã£ã‹ã‹ã£ã¦ã—ã¾ã£ãŸã®ã‹ã‚‚ã—ã‚Œãªã„ã¨æ€ã£ãŸãŒã€‚
- ãŸã ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ï¼’ã¤ã«æ¸›ã‚‰ã—ã¦ã‚‚ã€ãã‚Œã§ã‚‚å‡¦ç†ãŒæ­¢ã¾ã£ã¦ã—ã¾ã£ãŸã®ã§ã€ã„ã¾ã„ã¡é‡ˆç„¶ã¨ã—ãªã„
- ãã“ã§ã€Google API ã‚’ä½¿ã£ã¦ã¿ã‚‹ã“ã¨ã«ã—ãŸ

# Google API ã‚’ä½¿ã£ã¦ã¿ã‚‹

- ã“ã¡ã‚‰ã®è¨˜äº‹ã‚’å‚è€ƒã«ã€Cloud Workflow ã®ä¸­ã§ã€Google API ã‚’ä½¿ã£ã¦ CSV ã‚’èª­ã¿è¾¼ã‚“ã 

https://book.st-hakky.com/hakky/from-big-query-to-cloud-sql/

- **ğŸš¨æ³¨æ„1**ï¸ ã“ã‚Œã§è¨˜äº‹ã‚’æ›¸ã“ã†ã¨ã‚‚æ€ã£ãŸãã‚‰ã„ã ãŒã€APIçµŒç”±ã§ CSV ã‚’ Cloud SQL ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹éš›ã«ã¯ã€APIã‚’å®Ÿè¡Œã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã ã‘ã§ã¯ãªãã€**Cloud SQL ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ã‚‚ã€ãƒã‚±ãƒƒãƒˆã«å¯¾ã—ã¦é©åˆ‡ãªæ¨©é™ã‚’ä¸ãˆã‚‹å¿…è¦ãŒã‚ã‚‹**ï¼ˆ[ã‚½ãƒ¼ã‚¹](https://cloud.google.com/sql/docs/postgres/import-export/import-export-csv?hl=ja#required_roles_and_permissions_for_importing_to)ï¼‰
    - è‡ªåˆ†ã¯ã“ã“ã«è©°ã¾ã£ã¦ã€ã ã„ã¶ã‚“æ™‚é–“ã‚’æº¶ã‹ã—ã¦ã—ã¾ã£ãŸ...
- **ğŸš¨æ³¨æ„2**ï¸ APIçµŒç”±ã§CSVã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆã€CSVã«ãƒ˜ãƒƒãƒ€ãƒ¼ã¯å«ã‚ãªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨
  - APIçµŒç”±ã ã¨ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç„¡è¦–ã™ã‚‹ã¨ã„ã£ãŸã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒãªã„
  - ãªã®ã§ã€å‡ºåŠ›æ™‚ã®ã‚¹ã‚­ãƒ¼ãƒã‚’åˆ¥ã§ç®¡ç†ã—ã¦ãŠã„ã¦ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«ãã®ã‚¹ã‚­ãƒ¼ãƒã‚’æŒ‡å®šã™ã‚‹ã‚ˆã†ã«ã—ãŸ
- ä»¥ä¸‹ã¯ã€Cloud Workflow ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ã‚³ãƒ¼ãƒ‰

```yml
main:
  params: [event]
  steps:
    - init:
        assign:
          - projectId: 'project-id'
          - instance: 'instance-id'
          - datasetId: 'dataset-id'
          - tableId: 'table-id'
          - bucket: ${event.bucket}
    - log_event:
        call: sys.log
        args:
          data: ${event}
    - list_csv_files_to_delete:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${bucket}
        result: files_to_delete
    - delete_csv_files_if_exist:
        for:
          value: file
          in: ${files_to_delete.items}
          steps:
            - delete_file:
                call: googleapis.storage.v1.objects.delete
                args:
                  bucket: ${bucket}
                  # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹å ´åˆã€URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãªã„ã¨404ã‚¨ãƒ©ãƒ¼ã«ãªã£ã¦ã—ã¾ã†ã“ã¨ã«æ³¨æ„
                  # https://cloud.google.com/workflows/docs/reference/googleapis/storage/v1/objects/delete
                  object: ${text.url_encode(file.name)}
    - assign_match_glob:
        assign:
          - csvMatchGlob: "*.csv"
    - export_csv_from_bq:
        call: googleapis.bigquery.v2.jobs.insert
        args:
          projectId: ${projectId}
          body:
            configuration:
              extract:
                sourceTable:
                  projectId: ${projectId}
                  datasetId: ${datasetId}
                  tableId: ${tableId}
                destinationUri: ${"gs://" + bucket + "/" + csvMatchGlob}
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã¯å«ã‚ãªã„
                printHeader: false
    - count_storage_files:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${bucket}
          matchGlob: ${csvMatchGlob}
        result: listResult
    - check_if_files_exist:
        switch:
          - condition: ${"items" in listResult}
            next: import_csv_to_sql
        next: no_export
    - import_csv_to_sql:
        call: list_file_to_import
        args:
            bucket: ${bucket}
            matchGlob: ${csvMatchGlob}
            projectId: ${projectId}
            instance: ${instance}
            databaseSchema: "postgres"
            importTable: "\"ImportTest\""
        next: finish
    - no_export:
        return: 'No CSV Export'
    - finish:
        return: ${listResult}

# è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹
list_file_to_import:
  params:
    - bucket
    - matchGlob
    - projectId
    - instance
    - databaseSchema
    - importTable
  steps:
    - list-files:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${bucket}
          matchGlob: ${matchGlob}
        result: list_result
    - process-files:
        for:
          value: file
          in: ${list_result.items}
          steps:
            - wait-import:
                call: import_file
                args:
                  projectId: ${projectId}
                  instance: ${instance}
                  databaseSchema: ${databaseSchema}
                  importTable: ${importTable}
                  file: ${"gs://" + bucket + "/" + file.name}
    - return-step:
        return: ${list_result}

# 1ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹
import_file:
  params:
    - projectId
    - instance
    - databaseSchema
    - importTable
    - file
  steps:
    - callImport:
        call: http.post
        args:
          url: ${"https://sqladmin.googleapis.com/v1/projects/" + projectId + "/instances/" + instance + "/import"}
          auth:
            type: OAuth2
          body:
            importContext:
              uri: ${file}
              database: ${databaseSchema}
              fileType: CSV
              # ã©ã®ã‚«ãƒ©ãƒ ã®é †ç•ªã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã‹ã‚’æŒ‡å®šã™ã‚‹
              csvImportOptions:
                table: ${importTable}
                columns:
                    - "id"
                    - "userId"
                    - "createdAt"
                    - "point"
        result: operation
    - chekoperation:
        switch:
          - condition: ${operation.body.status != "DONE"}
            next: wait
        next: completed
    - completed:
        return: "done"
    - wait:
        call: sys.sleep
        args:
          seconds: 1
        next: getoperation
    - getoperation:
        call: http.get
        args:
          url: ${operation.body.selfLink}
          auth:
            type: OAuth2
        result: operation
        next: chekoperation
```

- ã“ã®ã‚„ã‚Šæ–¹ã ã¨ã€æ™‚é–“ã¯ã‹ã‹ã£ã¦ã‚‚ã€æœ€å¾Œã¾ã§ Cloud SQLã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ãŒã§ããŸ
- Cloud Workflow ãªã‚‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚‚ãªã„ã®ã§ã€ä»®ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ã‚‚ã€ã“ã®ã‚„ã‚Šæ–¹ãªã‚‰å®‰å®šã—ãŸã‚¤ãƒ³ãƒãƒ¼ãƒˆåŸºç›¤ã«ãªã‚Šãã†
- ä»¥ä¸‹ã¯ä»Šå›ã®è¨ˆæ¸¬çµæœ
    | Cloud SQLã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ§‹æˆ | ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚é–“ |
    |------------------|----------------|
    | 2 vCPUãƒ»8GBãƒ¡ãƒ¢ãƒª | ç´„1æ™‚é–“30åˆ†    |
    | 4 vCPUãƒ»16GBãƒ¡ãƒ¢ãƒª| ç´„1æ™‚é–“        |
    

# ãŠã‚ã‚Šã«
- å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’ Cloud SQL ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹è¨˜äº‹ãŒå°‘ãªã‹ã£ãŸã®ã§è‹¦åŠ´ã—ãŸã®ã§ã€ã“ã®è¨˜äº‹ãŒèª°ã‹ã®åŠ©ã‘ã«ãªã‚Œã°å¬‰ã—ã„ã§ã™ï¼

---
title: "【自由研究】Cloud Run JobsでCloud SQLにCSV importするのにかかった時間とスペックについて"
emoji: "🍉"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ['GCP', 'Firestore']
published: false
---

# 発端 🎐
- Cloud StorageにCSVファイルをアップロードしたら、自動でCloud SQLにそのデータがインポートされる仕組みが欲しい！️💡
- でも正直どのくらいのデータで、どのくらいの時間がかかるのか、どのくらいのスペックが必要なのかがイマイチ検討つかない🤔
- じゃあ調べてみよう💪


# 注意事項 💰

- コスト削減のため、各実験は1回ずつのみ実施しております。
- 複数回の実行による平均値ではないため、結果に若干のばらつきがある可能性があります。
- より正確な結果を得るには複数回の実行が理想的ですが、今回はコスト面を考慮し、単発の実験結果となっております。

この点をご了承いただいた上で、以降の実験結果をご覧ください。限られたデータではありますが、おおよその傾向を把握する参考としてお役立ていただければ幸いです。



# 実験内容
- 以下のスキーマをCloud SQL（PostgreSQL）に作ります

```
model ImportTest {
  id Int @id @default(autoincrement())
  postId String
  userId Int
  point Int

  @@unique([postId, userId])
}
```

- Cloud Storage にCSVをアップロードすると、Cloud Run Jobsが実行され、Cloud SQLにインポートされるワークフローを作っておきます
  - 今回はこのワークフローの実装は割愛します。ざっくりと以下の流れを実装しています

  ```mermaid
  graph LR
    A[CSVファイルをStorageにアップロード] --> B[Eventarc]
    B --> C[Cloud Workflow]
    C --> D[Cloud Run Jobs]
    D --> E[Cloud SQL]
  ```
::::details Cloud Run Jobsの中身

:::message
Dockerコンテナは、Artifact Registryを利用しています

```sh
# プロジェクト構成
.
├── Dockerfile
├── index.ts
├── package.json
└── tsconfig.json
```
:::

:::details Dockerfile
```Dockerfile
FROM node:20-slim AS build

WORKDIR /app

COPY package*.json ./

RUN npm install

COPY . .

RUN npm run build

# マルチステージ
FROM node:20-slim

WORKDIR /app

COPY package*.json ./

RUN npm install --production

COPY --from=build /app/dist ./dist

CMD ["node", "dist/index.js"]
```

- ビルド & プッシュ
```sh
# Artifact Registryは x86_64 のみ対応
$ docker build --platform linux/amd64 -t {REGION}-docker.pkg.dev/{PROJECT_ID}/images/{IMAGE_NAME} .

$ docker push {REGION}-docker.pkg.dev/{PROJECT_ID}/images/{IMAGE_NAME}
```
:::


:::details index.ts
```ts
import admin from 'firebase-admin'
import { parse } from 'csv-parse'
import pg from 'pg'

const connectionConfig: pg.PoolConfig = {
  host: process.env.PG_HOST,
  port: 5432,
  user: process.env.PG_USER,
  password: process.env.PG_PASSWORD,
  database: process.env.PG_DATABASE,
}

admin.initializeApp()

// Eventarcから受け取ったCSV情報を表示したい
const bucketName = process.argv[2]
const fileName = process.argv[3]
const contentType = process.argv[4]

const main = async () => {
  if (!bucketName || !fileName || !contentType) {
    console.error('Invalid arguments')
    process.exit(1)
  }

  if (contentType !== 'text/csv') {
    console.error('contentType should be text/csv, but got', contentType)
    process.exit(1)
  }

  if (admin.app().options.credential) {
    console.log('Firebase Admin SDK is initialized and authenticated.')
  } else {
    console.error('Firebase Admin SDK initialization or authentication failed.')
    process.exit(1)
  }

  console.time(`${fileName}から、Cloud SQLへのインポート`)

  const bucket = admin.storage().bucket(bucketName)

  const pool = new pg.Pool(connectionConfig)

  const csvFile = bucket.file(fileName)
  const csvParser = parse({
    columns: true,
    skip_empty_lines: true,
  })

  let totalRowCount = 0

  csvFile
    .createReadStream()
    .pipe(csvParser)
    .on('readable', async () => {
      const client = await pool.connect()

      client.on('error', (err) => {
        console.error('Error connecting to Cloud SQL:!!', err)
      })

      try {
        // プールからクライアントを取得して、簡単なクエリを実行
        await client.query('BEGIN')
        let record
        let rowCount = 0
        while ((record = csvParser.read())) {
          // 1行ずつ読み込み
          const query = {
            text: 'INSERT INTO "ImportTest" ("userId", "postId", point) VALUES ($1, $2, $3) ON CONFLICT ("userId", "postId") DO UPDATE SET point = $3',
            values: [record.userId, record.postId, record.point],
          }
          await client.query(query)

          rowCount++
          totalRowCount++
        }

        await client.query('COMMIT')

        console.log(`${rowCount}行のデータをCloud SQLにインポートしました。`)
      } catch (err) {
        await client.query('ROLLBACK')
        console.error('Error connecting to Cloud SQL:', err)
      } finally {
        client.release() // 接続をプールに返却
      }
    })
    .on('error', (err) => {
      console.error('CSVパースエラー:', err)
    })
    .on('end', async () => {
      await pool.end() // 接続プールを終了
      console.log('Pool.end called!')
      console.log(`${totalRowCount}行のデータをCloud SQLにインポートしました。`)
      console.timeEnd(`${fileName}から、Cloud SQLへのインポート`)
    })

}

main()

```
:::

:::details Cloud Workflowの設定
- [ソース](https://cloud.google.com/workflows/docs/create-workflow-console?hl=ja)
```yaml
main:
  params: [event]
  steps:
    - init:
        assign:
          - bucket: ${event.bucket}
          - filename: ${event.data.name}
          - contentType: ${event.data.contentType}
    - log_event:
        call: sys.log
        args:
          data: ${event}
    - run:
        call: googleapis.run.v2.projects.locations.jobs.run
        args:
          name: "projects/{PROJECT_ID}/locations/{LOCATION}/jobs/{JOB_NAME}"
          body:
            overrides:
              containerOverrides:
                - args:
                    - "node"
                    - "dist/index.js"
                    - ${bucket}
                    - ${filename}
                    - ${contentType}
    - finish:
        return: "ok"

```
:::

::::
  
# 比較するもの
- 以下の３つです
1. CSVファイル
2. Cloud Run Jobsのスペック
3. Cloud SQLのスペック

## CSVファイル
- Stoargeに、サイズの異なるCSVファイルをアップロードします

  | ファイル名 | 行数 | データ量（目安） |
  |------------|------|----------|
  | 100_000.csv     | 100,000 | 5.5MB |
  | 500_000.csv     | 500,000 | 27.5MB |
  | 1_000_000.csv     | 1,000,000 | 55MB |
  | 10_000_000.csv     | 10,000,000 | 550MB |
  | 100_000_000.csv    | 100,000,000 | 5.5GB |

- Cloud Run Jobsの

- Cloud Run Jobs のスペックと、

# 実験開始

## 実験１: 500,000行のCSVをインポートするのにかかった時間とスペック

- 

| 項目 | 仕様 |
|------|------|
| Cloud SQL のスペック | vCPU 2, メモリ 8GB, SSD 20GB |
| CSVファイル | 500_000.csv (500,000行、約27.5MB) |

結果

| メモリ | CPU | 実行時間 (m:ss.mmm) |
|--------|-----|----------|
| 2GiB   | 1   | 8:32.279 |
| 4GiB   | 1   | 4:33.654 |
| 4GiB   | 2   | 5:22.256 |
| 8GiB   | 2   | 4:26.819 |

わかったこと
- CPUよりもメモリの影響が大きい
- 500,000行程度のインポートの場合、メモリ4GiB、CPU 1コアの構成で十分な性能が得られます。これ以上のスペックアップは効果が薄く、コストパフォーマンスが低下する傾向にあります。

## 実験２: CSVデータ量と Cloud SQLのスペック

- Cloud Run Jobs のスペックを

| メモリ | CPU |
|--------|-----|
| 4GiB   | 2   |

に固定し、CSVデータ量とCloud SQLのスペックを変えて実験を行いました。結果は以下の通りです：

| CSVファイル | データ量（目安） | Cloud SQL vCPU | Cloud SQL メモリ | 実行時間 (m:ss.mmm) |
|-------------|----------|-----------------|------------------|---------------------|
| 100_000.csv | 27.5MB | 2 | 8GB | 2.10.599 |
| 500_000.csv | 27.5MB | 2 | 8GB | 4.33.654 |
| 1_000_000.csv | 55MB | 2 | 8GB | |
| 10_000_000.csv | 550MB | 2 | 8GB |  |

わかったこと：
- データ量が増えるにつれて、実行時間は概ね線形に増加します。
- 1,000,000行（55MB）までは、比較的小さなCloud SQLインスタンス（vCPU 1, メモリ 3.75GB）で十分処理できます。
- 10,000,000行（550MB）以上のデータになると、より大きなCloud SQLインスタンスが必要になります。
- 100,000,000行（5.5GB）のような大規模データの場合、かなり強力なCloud SQLインスタンスと長い処理時間が必要になります。

これらの結果から、データ量に応じて適切なCloud SQLのスペックを選択することが重要であることがわかります。小規模から中規模のデータ（100万行程度まで）であれば、比較的小さなインスタンスで効率的に処理できますが、大規模データになるとスペックアップが必要になります。



# 余談
- もともとは Firestore で上をやろうと思っていたのですが、100,000件の書き込み当たり、$0.115かかるんですよね
- 仮に一億書き込むとなったら、一回の実験で$115（16656円）飛んでいくので、笑えたもんじゃない😓
[料金](https://firebase.google.com/docs/firestore/pricing?hl=ja#select-region)
- そう考えると Cloud SQL の方が安いですね。
